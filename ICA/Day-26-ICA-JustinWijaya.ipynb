{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to successfully complete this assignment you need to participate both individually and in groups during class.  Have one of the instructors check your notebook and sign you out before leaving class. Turn in your assignment using. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-Class Assignment: Understanding Scaling\n",
    "\n",
    "\n",
    "<img alt=\"Graph showing how different programs scale with additional processors.\" src=\"http://web.eecs.utk.edu/~huangj/hpc/fig_speedup.png\">\n",
    "\n",
    "Image From: http://web.eecs.utk.edu/~huangj/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agenda for today's class (70 minutes)\n",
    "\n",
    "1. (15 minutes) [Homework 3 Discussion](#HW3)\n",
    "2. (20 minutes)[Introduction to Homework 4](#Introduction-to-Homework-4)\n",
    "3. (25 minutes) [Amdahl's law](#Amdahls-law)\n",
    "4. (25 minutes)[Strong vs Weak Scaling](#Strong-vs-Weak-Scaling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=HW3></a>\n",
    "# 1. HW 3 Review\n",
    "\n",
    "With your group, discuss how HW 3 went. Discuss what strategies you had to use to get the code working. Discuss what you learned or what were the biggest challenges. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "<a name=Introduction-to-Homework-4></a>\n",
    "# 2. Introduction to Homework 4\n",
    "\n",
    "Take a look at the homework 4 posted to the course website. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "<a name=Amdahls-law></a>\n",
    "# 3. Amdahl's law\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The following examples come from here: https://en.wikipedia.org/wiki/Amdahl%27s_law\n",
    "\n",
    "Amdahl's law can be formulated in the following way:\n",
    "\n",
    "$$S_\\text{latency}(s)=\\frac {1}{(1-p)+{\\frac {p}{s}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$S_\\text{latency}$ is the theoretical speedup of the execution of the whole task;\n",
    "\n",
    "- $s$ is the speedup of the part of the task that benefits from improved system resources (i.e. running in parallel);\n",
    "- p is the proportion of execution time that the part benefiting from improved resources originally occupied.\n",
    "\n",
    "Furthermore,\n",
    "\n",
    "$${\\displaystyle {\\begin{cases}S_{\\text{latency}}(s)\\leq {\\dfrac {1}{1-p}}\\\\[8pt]\\lim \\limits _{s\\to \\infty }S_{\\text{latency}}(s)={\\dfrac {1}{1-p}}.\\end{cases}}} $$\n",
    "\n",
    "If 30% of the execution time may be the subject of a speedup, $p$ will be 0.3; if the improvement makes the affected part twice as fast, $s$ will be 2. Amdahl's law states that the overall speedup of applying the improvement will be:\n",
    "\n",
    "$${\\displaystyle S_{\\text{latency}}={\\frac {1}{1-p+{\\frac {p}{s}}}}={\\frac {1}{1-0.3+{\\frac {0.3}{2}}}}=1.18.}$$\n",
    "\n",
    "For example, assume that we are given a serial task which is split into four consecutive parts, whose percentages of execution time are p1 = 0.11, p2 = 0.18, p3 = 0.23, and p4 = 0.48 respectively. Then we are told that the 1st part is not sped up, so s1 = 1, while the 2nd part is sped up 5 times, so s2 = 5, the 3rd part is sped up 20 times, so s3 = 20, and the 4th part is sped up 1.6 times, so s4 = 1.6. By using Amdahl's law, the overall speedup is\n",
    "\n",
    "$${\\displaystyle S_{\\text{latency}}={\\frac {1}{{\\frac {p1}{s1}}+{\\frac {p2}{s2}}+{\\frac {p3}{s3}}+{\\frac {p4}{s4}}}}={\\frac {1}{{\\frac {0.11}{1}}+{\\frac {0.18}{5}}+{\\frac {0.23}{20}}+{\\frac {0.48}{1.6}}}}=2.19} $$\n",
    "\n",
    "Notice how the 5 times and 20 times speedup on the 2nd and 3rd parts respectively don't have much effect on the overall speedup when the 4th part (48% of the execution time) is accelerated by only 1.6 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>QUESTION:</font>** Consider the following example.  A researcher has a simulation code that takes a a whole week to run (168 hours).  One of the slowest parts of the code is reading in the input data.  The researcher believes that in the best case, MPI could increase the speed linearly with the number of processors. For example, if the code is given 500 cores it will run 500 times faster.  Assuming that reading in the data takes 10% of the overall runtime.  How much faster will the parallel run on 500 cores?  What is the fastest possible run time? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.110864252388358 times faster\n",
      "15.123360000000002 hours\n"
     ]
    }
   ],
   "source": [
    "S = 1 / (1 - .1 + (.1 / 500))\n",
    "print(S, \"times faster\")\n",
    "print((168 * .1) / S, \"hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>QUESTION:</font>** What is the theoretical fastest possible run time for the entire project that could be achieved with by optimizing this region of the code? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152.87664 hours\n"
     ]
    }
   ],
   "source": [
    "print(168 - (168 * .1) / S, \"hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>QUESTION:</font>** Now assume that the program reads in new data every iteration so that the parallel region now takes up 60% of the overall runtime.  How much faster will the parallel code run on 500 cores?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4925224327018944 times faster\n"
     ]
    }
   ],
   "source": [
    "S = 1 / (1 - .6 + (.6 / 500))\n",
    "print(S, \"times faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>QUESTION:</font>**  What is the theoretical fastest possible run time for the entire project that could be achieved with by optimizing this region of the code? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127.55904000000001 hours\n"
     ]
    }
   ],
   "source": [
    "print(168 - (168 * .6) / S, \"hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "<a name=Strong-vs-Weak-Scaling></a>\n",
    "# 4. Strong vs Weak Scaling\n",
    "\n",
    "\n",
    "> **STRONG SCALING** In this case the problem size stays fixed but the number of processing elements are increased. This is used as justification for programs that take a long time to run (something that is cpu-bound). The goal in this case is to find a \"sweet spot\" that allows the computation to complete in a reasonable amount of time, yet does not waste too many cycles due to parallel overhead. In strong scaling, a program is considered to scale linearly if the speedup (in terms of work units completed per unit time) is equal to the number of processing elements used ( N ). In general, it is harder to achieve good strong-scaling at larger process counts since the communication overhead for many/most algorithms increases in proportion to the number of processes used.\n",
    "> \n",
    "> If the amount of time to complete a work unit with 1 processing element is $t_1$, and the amount of time to complete the same unit of work with $N$ processing elements is $t_N$, the strong scaling efficiency (as a percentage of linear) is given as:\n",
    "\n",
    "$$ \\frac{t_1}{( N * t_N )} * 100%$$\n",
    "\n",
    "\n",
    "> **WEAK SCALING** In this case the problem size (workload) assigned to each processing element stays constant and additional elements are used to solve a larger total problem (one that wouldn't fit in RAM on a single node, for example). Therefore, this type of measurement is justification for programs that take a lot of memory or other system resources (something that is memory-bound). In the case of weak scaling, linear scaling is achieved if the run time stays constant while the workload is increased in direct proportion to the number of processors. Most programs running in this mode should scale well to larger core counts as they typically employ nearest-neighbour communication patterns where the communication overhead is relatively constant regardless of the number of processes used; exceptions include algorithms that employ heavy use of global communication patterns, eg. FFTs and transposes.\n",
    ">\n",
    "> If the amount of time to complete a work unit with 1 processing element is $t_1$, and the amount of time to complete $N$ of the same work units with $N$ processing elements is $t_N$, the weak scaling efficiency (as a percentage of linear) is given as:\n",
    "\n",
    "$$ \\frac{t_1}{t_N} * 100% $$\n",
    "\n",
    "\n",
    "from: https://www.sharcnet.ca/help/index.php/Measuring_Parallel_Scaling_Performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>QUESTION:</font>**  Consider the example from HW3.  Do you think this program represents strong or week scaling?  What about the new problem in HW 4? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3 represents strong scaling because we are taking an array and splitting up the computations through the use of multiple processors.\n",
    "- 4 represents strong scaling as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "-----\n",
    "### Congratulations, we're done!\n",
    "\n",
    "Have one of the instructors check your notebook and sign you out before leaving class. Turn in your assignment using D2L."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written by Dr. Dirk Colbry, Michigan State University (Updated by Dr. Nathan Haut in Spring 2025)\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-NonCommercial 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
